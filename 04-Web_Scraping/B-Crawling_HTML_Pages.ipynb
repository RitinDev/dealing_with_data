{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Crawling HTML Pages",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/04-Web_Scraping/B-Crawling_HTML_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMIH8iiQkaU"
      },
      "source": [
        "# Crawling and Extracting Data from Websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYNQSVkqQkaV"
      },
      "source": [
        "# !sudo -H apt-get -y install libxml2-dev libxslt-dev python3-dev\n",
        "# !sudo -H pip3 install -U lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNLZmq2gQkag"
      },
      "source": [
        "## Searching in HTML: Fetching the webpage title from ESPN.com\n",
        "\n",
        "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCx_Tvm3Qkah"
      },
      "source": [
        "import requests # This command allows us to fetch URLs\n",
        "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
        "import pandas # To create a dataframe\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.espn.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzkt9KXlQkal"
      },
      "source": [
        "doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3szL2NDkQkap"
      },
      "source": [
        "Let's start by getting the content of the `<title>` node from the site:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zude0Ys5Qkap"
      },
      "source": [
        "# This will return back a list of nodes that match\n",
        "# are called <title>......</title>\n",
        "results = doc.xpath('//title/text()')\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9-lPWltQkau"
      },
      "source": [
        "results[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70sDo6dQkaz"
      },
      "source": [
        "results = doc.xpath('//title')\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyuV2iSzQka2"
      },
      "source": [
        "# gives only the text stored directly under the node\n",
        "results[0].text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MVxeFfQka4"
      },
      "source": [
        "# gives all the text stored directly under the node *and* its children\n",
        "# for the <title> node, it does not make a difference\n",
        "results[0].text_content() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBhnl3zoQka7"
      },
      "source": [
        "# Compare the two results to see the difference\n",
        "# The first one gives all the text stored under the root <HTML> node\n",
        "# The second gives only the text immediately under HTML (but not under the children)\n",
        "# doc.xpath(\"/html\")[0].text_content()\n",
        "# doc.xpath(\"/html\")[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVUdtBuVTcUv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "* Connect to the NYU Stern website, and fetch the title of the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e4dq_o-Ti7U"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V0uJ9qmTjDw"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBSnvmd0Qka-"
      },
      "source": [
        "stern_url = 'http://www.stern.nyu.edu'\n",
        "stern_html = requests.get(stern_url).text\n",
        "stern_doc = html.fromstring(stern_html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyBZUlIfQkbB"
      },
      "source": [
        "title1 = stern_doc.xpath(\"//title/text()\")\n",
        "title1[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2gpzcYTQkbE"
      },
      "source": [
        "title2 = stern_doc.xpath(\"//title\")\n",
        "title2[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHjsVOSvTnET"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6aTxluTnlS"
      },
      "source": [
        "## Searching for elements of interest in the web page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNzG7ZY9QkbH"
      },
      "source": [
        "The `doc` variable is an `HtmlElement` object, and we can now use **XPath** queries to locate the elements that we need. (Depending on time, we may do in class a tutorial on XPath. For now, you can look at the [W3Schools tutorial](http://www.w3schools.com/xpath/xpath_nodes.asp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTvqXmNQkbI"
      },
      "source": [
        "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-G5eLsCQkbM"
      },
      "source": [
        "links = doc.xpath(\"//a\")\n",
        "len(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzNj3eZ4QkbY"
      },
      "source": [
        "# Let's pick now a random link\n",
        "\n",
        "lnk = links[80]\n",
        "type(lnk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwPXAb1Qkbe"
      },
      "source": [
        "The `lnk` variable is again an HtmlElement. To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text` method (to get the text within the `<a>...</a>` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvhzBi6cQkbf"
      },
      "source": [
        "lnk.get(\"href\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSPOFRDyQkbl"
      },
      "source": [
        "lnk.text_content()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbdACF-Qkbp"
      },
      "source": [
        "# The strip() removes blank spaces before and after the text\n",
        "lnk.text_content().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "eO2mcITht8gx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaj4BbALQkbS"
      },
      "source": [
        "links = doc.xpath(\"//a\")\n",
        "# Iterates over all the links (this means all the nodes\n",
        "# that matched the //a XPath query) and prints the content\n",
        "# of the attribute href and the text for that node\n",
        "for link in links:\n",
        "    print(\"==================================\")\n",
        "    print(link.get(\"href\"), \"==>\", link.text_content().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ4eRqTBQkbr"
      },
      "source": [
        "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Vz3APqQkbs"
      },
      "source": [
        "urls = [lnk.get(\"href\") for lnk in doc.xpath('//a')]\n",
        "urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNgg6P7HQkbv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use a list compresension approach, to get the text_content of all the URLs in the page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0K4iQ82Qkbv"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv8k8jCUQkby"
      },
      "source": [
        "And now create a list where we put together text content and the URL for each link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYN_XbBPRTiy"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhJIYxiDRU2f"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYw-F6zPRSzN"
      },
      "source": [
        "text = [lnk.text_content().strip() for lnk in doc.xpath(\"//a\")]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnRT3ecxQkbz"
      },
      "source": [
        "# Creating a list of tuples where we put together href and text for each link\n",
        "list_tuples = [(lnk.get(\"href\"), lnk.text_content().strip()) for lnk in doc.xpath(\"//a\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGMfS0tHQkb1"
      },
      "source": [
        "# Creating a list of dictionaries with the text and URL for each link\n",
        "list_dicts = [{\"URL\": lnk.get(\"href\"), \"Text\": lnk.text_content().strip()} for lnk in doc.xpath(\"//a\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7VO7birqQkb4"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(list_dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eyJ4fu-Qkb7"
      },
      "source": [
        "### More Advanced Example: Get the list of headlines from ESPN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTPkHGnQkb7"
      },
      "source": [
        "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using the appropriate XPath queries, we will get what we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_recHuQkb9"
      },
      "source": [
        "Let's start by fetching the page, and parsing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueC9w3oAQkb-"
      },
      "source": [
        "url = \"http://www.espn.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H9-IZeEQkcB"
      },
      "source": [
        "By using the `\"Right-Click > Inspect\"` option of Chrome,\n",
        "we right click on the headlines and select `\"Inspect\"`.\n",
        "This opens the source code.\n",
        "There we see that all under a `<div class=\"headlineStack\">` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEd6g0HJQkcI"
      },
      "source": [
        "headlineNode = doc.xpath('//div[@class=\"headlineStack\"]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YE3ytU0QkcK"
      },
      "source": [
        "The result of that operation is a list with 8 elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrhr3KS0QkcL"
      },
      "source": [
        "type(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJSB5D2VQkcO"
      },
      "source": [
        "len(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq1X3MtdQkcT"
      },
      "source": [
        "Each headline is under a  `<li><a href=\"....\"></a>` tag.\n",
        "So, we get all the `<li><a ...>` tags within the `<div class=\"headlineStack\">`\n",
        "(which is stored in the \"`headlineNode`\" variable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-n8Ajb5QkcU"
      },
      "source": [
        "headlines = headlineNode[0].xpath('.//li/a')\n",
        "len(headlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StKUBexQkcX"
      },
      "source": [
        "Now, we have the nodes with the conent in the headlines variable.\n",
        "We extract the text and the URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ALKiqXwQkcX"
      },
      "source": [
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRzf_DJMQkca"
      },
      "source": [
        "And let's create our dataframe, so that we can have a better view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bki_w9yoQkca"
      },
      "source": [
        "dataframe = pandas.DataFrame(data)\n",
        "dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbWeA2wCQkce"
      },
      "source": [
        "#### Of course, there are always more than one way to skin a cat...\n",
        "\n",
        "Alternatively, if we did not want to restrict ourselves to just the first headline box, we could write an alternative query, to get back all the headlines, that appear in an XPath `//div[@class=\"headlineStack\"]//li/a`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxxXLHsIQkcf"
      },
      "source": [
        "headlines = doc.xpath('//div[@class=\"headlineStack\"]//li/a')\n",
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrHkZwQQkci"
      },
      "source": [
        "headlines = doc.xpath('//a[@data-mptype=\"headline\"]')\n",
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3SgwTKOQkck"
      },
      "source": [
        "## In Class Example: Crawl BuzzFeed\n",
        "\n",
        "* We will try to get the top articles that appear on Buzzfeed\n",
        "* We will grab the link for the article, the text of the title, the description, and the editor.\n",
        "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOYRZpEQkcm"
      },
      "source": [
        "#your code here\n",
        "import requests\n",
        "\n",
        "from lxml import html\n",
        "\n",
        "resp = requests.get(\"http://www.buzzfeed.com\")\n",
        "doc = html.fromstring(resp.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nei56OAhSfcl"
      },
      "source": [
        "story_nodes = doc.xpath('//article[@data-buzzblock=\"story-card\"]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Xwb94eTEiH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9eDO7QjTEvK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uNeduCfSQkcu"
      },
      "source": [
        "#### Solution for Buzzfeed (as of Septenber 22, 2022)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKr3oHlGQkcv"
      },
      "source": [
        "import requests # This command allows us to fetch URLs\n",
        "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
        "import pandas\n",
        "import re\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.buzzfeed.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document\n",
        "\n",
        "articleNodes = doc.xpath(\"//div[@class='feedItem_textWrapper__S6osO']\") \n",
        "\n",
        "def parseArticleNode(article):\n",
        "    headline = article.xpath(\".//h2\")[0].text_content().strip()\n",
        "    headline_link = article.xpath(\".//a\")[0].get(\"href\")\n",
        "\n",
        "    editor_node = article.xpath(\".//div[@class='xs-text-6 text-gray xs-mt1']\")\n",
        "    if len(editor_node)>0:\n",
        "      editor = editor_node[0].text_content().strip()\n",
        "    else:\n",
        "      editor = \"\"\n",
        "    regex = re.compile(r'^by (.*)$')\n",
        "    matches = regex.finditer(editor)\n",
        "    for m in matches:\n",
        "      editor = m.group(1)\n",
        "\n",
        "\n",
        "    result = {\n",
        "        \"headline\": headline,\n",
        "        \"URL\" : headline_link,\n",
        "        \"editor\" : editor\n",
        "    }\n",
        "    return result\n",
        "\n",
        "data = [parseArticleNode(article) for article in articleNodes]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}