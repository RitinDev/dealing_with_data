{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Crawling HTML Pages",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/04-Web_Scraping/B-Crawling_HTML_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMIH8iiQkaU"
      },
      "source": [
        "# Crawling and Extracting Data from Websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYNQSVkqQkaV"
      },
      "source": [
        "# !sudo -H apt-get -y install libxml2-dev libxslt-dev python3-dev\n",
        "# !sudo -H pip3 install -U lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNLZmq2gQkag"
      },
      "source": [
        "## Searching in HTML: Fetching the webpage title from ESPN.com\n",
        "\n",
        "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCx_Tvm3Qkah"
      },
      "source": [
        "import requests # This command allows us to fetch URLs\n",
        "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
        "import pandas # To create a dataframe\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.espn.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzkt9KXlQkal"
      },
      "source": [
        "doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3szL2NDkQkap"
      },
      "source": [
        "Let's start by getting the content of the `<title>` node from the site:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zude0Ys5Qkap"
      },
      "source": [
        "# This will return back a list of nodes that match\n",
        "# are called <title>......</title>\n",
        "results = doc.xpath('//title/text()')\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9-lPWltQkau"
      },
      "source": [
        "results[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70sDo6dQkaz"
      },
      "source": [
        "results = doc.xpath('//title')\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyuV2iSzQka2"
      },
      "source": [
        "# gives only the text stored directly under the node\n",
        "results[0].text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MVxeFfQka4"
      },
      "source": [
        "# gives all the text stored directly under the node *and* its children\n",
        "# for the <title> node, it does not make a difference\n",
        "results[0].text_content() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBhnl3zoQka7"
      },
      "source": [
        "# Compare the two results to see the difference\n",
        "# The first one gives all the text stored under the root <HTML> node\n",
        "# The second gives only the text immediately under HTML (but not under the children)\n",
        "# doc.xpath(\"/html\")[0].text_content()\n",
        "# doc.xpath(\"/html\")[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVUdtBuVTcUv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "* Connect to the NYU Stern website, and fetch the title of the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e4dq_o-Ti7U"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V0uJ9qmTjDw"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBSnvmd0Qka-"
      },
      "source": [
        "stern_url = 'http://www.stern.nyu.edu'\n",
        "stern_html = requests.get(stern_url).text\n",
        "stern_doc = html.fromstring(stern_html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyBZUlIfQkbB"
      },
      "source": [
        "title1 = stern_doc.xpath(\"//title/text()\")\n",
        "title1[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2gpzcYTQkbE"
      },
      "source": [
        "title2 = stern_doc.xpath(\"//title\")\n",
        "title2[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHjsVOSvTnET"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6aTxluTnlS"
      },
      "source": [
        "## Searching for elements of interest in the web page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNzG7ZY9QkbH"
      },
      "source": [
        "The `doc` variable is an `HtmlElement` object, and we can now use **XPath** queries to locate the elements that we need. (Depending on time, we may do in class a tutorial on XPath. For now, you can look at the [W3Schools tutorial](http://www.w3schools.com/xpath/xpath_nodes.asp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTvqXmNQkbI"
      },
      "source": [
        "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-G5eLsCQkbM"
      },
      "source": [
        "links = doc.xpath(\"//a\")\n",
        "len(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzNj3eZ4QkbY"
      },
      "source": [
        "# Let's pick now a random link\n",
        "\n",
        "lnk = links[80]\n",
        "type(lnk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwPXAb1Qkbe"
      },
      "source": [
        "The `lnk` variable is again an HtmlElement. To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text` method (to get the text within the `<a>...</a>` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvhzBi6cQkbf"
      },
      "source": [
        "lnk.get(\"href\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSPOFRDyQkbl"
      },
      "source": [
        "lnk.text_content()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbdACF-Qkbp"
      },
      "source": [
        "# The strip() removes blank spaces before and after the text\n",
        "lnk.text_content().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "eO2mcITht8gx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaj4BbALQkbS"
      },
      "source": [
        "links = doc.xpath(\"//a\")\n",
        "# Iterates over all the links (this means all the nodes\n",
        "# that matched the //a XPath query) and prints the content\n",
        "# of the attribute href and the text for that node\n",
        "for link in links:\n",
        "    print(\"==================================\")\n",
        "    print(link.get(\"href\"), \"==>\", link.text_content().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ4eRqTBQkbr"
      },
      "source": [
        "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Vz3APqQkbs"
      },
      "source": [
        "urls = [lnk.get(\"href\") for lnk in doc.xpath('//a')]\n",
        "urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNgg6P7HQkbv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use a list compresension approach, to get the text_content of all the URLs in the page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0K4iQ82Qkbv"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv8k8jCUQkby"
      },
      "source": [
        "And now create a list where we put together text content and the URL for each link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYN_XbBPRTiy"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhJIYxiDRU2f"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYw-F6zPRSzN"
      },
      "source": [
        "text = [lnk.text_content().strip() for lnk in doc.xpath(\"//a\")]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnRT3ecxQkbz"
      },
      "source": [
        "# Creating a list of tuples where we put together href and text for each link\n",
        "list_tuples = [(lnk.get(\"href\"), lnk.text_content().strip()) for lnk in doc.xpath(\"//a\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGMfS0tHQkb1"
      },
      "source": [
        "# Creating a list of dictionaries with the text and URL for each link\n",
        "list_dicts = [{\"URL\": lnk.get(\"href\"), \"Text\": lnk.text_content().strip()} for lnk in doc.xpath(\"//a\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7VO7birqQkb4"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(list_dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eyJ4fu-Qkb7"
      },
      "source": [
        "### More Advanced Example: Get the list of headlines from ESPN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTPkHGnQkb7"
      },
      "source": [
        "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using the appropriate XPath queries, we will get what we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_recHuQkb9"
      },
      "source": [
        "Let's start by fetching the page, and parsing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueC9w3oAQkb-"
      },
      "source": [
        "url = \"http://www.espn.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H9-IZeEQkcB"
      },
      "source": [
        "By using the `\"Right-Click > Inspect\"` option of Chrome,\n",
        "we right click on the headlines and select `\"Inspect\"`.\n",
        "This opens the source code.\n",
        "There we see that all under a `<div class=\"headlineStack\">` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEd6g0HJQkcI"
      },
      "source": [
        "headlineNode = doc.xpath('//div[@class=\"headlineStack\"]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YE3ytU0QkcK"
      },
      "source": [
        "The result of that operation is a list with 8 elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrhr3KS0QkcL"
      },
      "source": [
        "type(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJSB5D2VQkcO"
      },
      "source": [
        "len(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq1X3MtdQkcT"
      },
      "source": [
        "Each headline is under a  `<li><a href=\"....\"></a>` tag.\n",
        "So, we get all the `<li><a ...>` tags within the `<div class=\"headlineStack\">`\n",
        "(which is stored in the \"`headlineNode`\" variable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-n8Ajb5QkcU"
      },
      "source": [
        "headlines = headlineNode[0].xpath('.//li/a')\n",
        "len(headlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StKUBexQkcX"
      },
      "source": [
        "Now, we have the nodes with the conent in the headlines variable.\n",
        "We extract the text and the URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ALKiqXwQkcX"
      },
      "source": [
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRzf_DJMQkca"
      },
      "source": [
        "And let's create our dataframe, so that we can have a better view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bki_w9yoQkca"
      },
      "source": [
        "dataframe = pandas.DataFrame(data)\n",
        "dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbWeA2wCQkce"
      },
      "source": [
        "#### Of course, there are always more than one way to skin a cat...\n",
        "\n",
        "Alternatively, if we did not want to restrict ourselves to just the first headline box, we could write an alternative query, to get back all the headlines, that appear in an XPath `//div[@class=\"headlineStack\"]//li/a`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxxXLHsIQkcf"
      },
      "source": [
        "headlines = doc.xpath('//div[@class=\"headlineStack\"]//li/a')\n",
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrHkZwQQkci"
      },
      "source": [
        "headlines = doc.xpath('//a[@data-mptype=\"headline\"]')\n",
        "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3SgwTKOQkck"
      },
      "source": [
        "## In Class Example: Crawl BuzzFeed\n",
        "\n",
        "* We will try to get the top articles that appear on Buzzfeed\n",
        "* We will grab the link for the article, the text of the title,  and the editor.\n",
        "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOYRZpEQkcm"
      },
      "source": [
        "#your code here\n",
        "import requests\n",
        "\n",
        "from lxml import html\n",
        "\n",
        "resp = requests.get(\"http://www.buzzfeed.com\")\n",
        "doc = html.fromstring(resp.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nei56OAhSfcl"
      },
      "source": [
        "story_nodes = doc.xpath('//article[@data-buzzblock=\"story-card\"]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Xwb94eTEiH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9eDO7QjTEvK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uNeduCfSQkcu"
      },
      "source": [
        "#### Solution for Buzzfeed (as of Septenber 22, 2022)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKr3oHlGQkcv",
        "outputId": "d73781dc-f6f9-446e-a288-176e20f3ae72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import requests # This command allows us to fetch URLs\n",
        "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
        "import pandas\n",
        "import re\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.buzzfeed.com/\"\n",
        "response = requests.get(url) # get the html of that url\n",
        "doc = html.fromstring(response.text) # parse it and create a document\n",
        "\n",
        "articleNodes = doc.xpath(\"//div[@class='feedItem_textWrapper__S6osO']\") \n",
        "\n",
        "def parseArticleNode(article):\n",
        "    headline = article.xpath(\".//h2\")[0].text_content().strip()\n",
        "    headline_link = article.xpath(\".//a\")[0].get(\"href\")\n",
        "\n",
        "    editor_node = article.xpath(\".//div[@class='xs-text-6 text-gray xs-mt1']\")\n",
        "    editor_text = editor_node[0].text_content() if len(editor_node)>0 else \"\"\n",
        "\n",
        "    regex = re.compile(r'^by (.*)$')\n",
        "    matches = list(regex.finditer(editor_text))\n",
        "    editor = matches[0].group(1) if len(matches)>0 else \"\"\n",
        "\n",
        "    result = {\n",
        "        \"headline\": headline,\n",
        "        \"URL\" : headline_link,\n",
        "        \"editor\" : editor\n",
        "    }\n",
        "    return result\n",
        "\n",
        "data = [parseArticleNode(article) for article in articleNodes]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             headline  \\\n",
              "0   47 Things You’ll Wish You’d Bought Last Fall B...   \n",
              "1   28 Actors Who Were Forced To Change Up Their L...   \n",
              "2   32 Pictures That Make Me Laugh Until I Pee My ...   \n",
              "3   Policy Changes, A Scam, And Child Predator All...   \n",
              "4        Why We All Owe Florence Pugh A Major Apology   \n",
              "5     Here Are 30 Tweets That Made Me Smile This Week   \n",
              "6   Can You Match The Character To The Nickelodeon...   \n",
              "7   People Are Sharing How They Respond When Someo...   \n",
              "8   Create An Autumn Bucket List To Find Out Which...   \n",
              "9   Reviewers Say They “Live In” These 29 Pieces O...   \n",
              "10  32 Upgrades To Make Your Household Feel More G...   \n",
              "11  40 Products That’ll Totally Change How You Put...   \n",
              "12  We Used AI To Show What “Percy Jackson” Charac...   \n",
              "13  The Truth About Marilyn Monroe And The Kennedy...   \n",
              "14       Which “GoT” Universe House Do You Belong In?   \n",
              "15  The Cast of Never Have I Ever Take the Co-Star...   \n",
              "16  Actor Nia Long Responds To Allegations Of Fian...   \n",
              "17  17 Screenshots Of Bosses That Show How Being I...   \n",
              "18  There’s No Way You’ve Seen 25/39 Of These Spoo...   \n",
              "19  Honestly, It’d Take A Miracle For You To Get 1...   \n",
              "20  You Know If You’re El Or Max, But Are You Sadi...   \n",
              "21  40 Then And Now Pictures Of When Celeb Couples...   \n",
              "22  “I Was Never So Happy To Leave A Job In My Lif...   \n",
              "23  Taylor Swift Has Had Many Iconic Eras, But Onl...   \n",
              "24  I Need Sophie Turner In More Comedies After Se...   \n",
              "25  Lamar Odom Shared His Reaction To Seeing Khloé...   \n",
              "26  What Dessert Should You Have Based On How Your...   \n",
              "27                                  Simple Kale Chips   \n",
              "28                              Homemade Dinner Rolls   \n",
              "29                          Pumpkin Spice Donut Holes   \n",
              "30                  We Tried The Spiciest Ramen in LA   \n",
              "31        “Don’t Worry Darling” Is Actually Just Fine   \n",
              "32  Honestly, It’d Take A Miracle For You To Get 1...   \n",
              "33  You Know If You’re El Or Max, But Are You Sadi...   \n",
              "34        Hey YOU! Got Opinions? Yeah, We Thought So.   \n",
              "35  “Don’t Worry Darling” Has Caused Quite A Stir ...   \n",
              "\n",
              "                                                  URL  \\\n",
              "0   https://www.buzzfeed.com/elizabethlilly/things...   \n",
              "1   https://www.buzzfeed.com/hannahmarder/makeover...   \n",
              "2   https://www.buzzfeed.com/daves4/pictures-that-...   \n",
              "3   https://www.buzzfeednews.com/article/kelseywee...   \n",
              "4   https://www.buzzfeed.com/hannahmarder/dont-wor...   \n",
              "5   https://www.buzzfeed.com/tori_honore/tweets-st...   \n",
              "6   https://www.buzzfeed.com/arianatorforever/nick...   \n",
              "7   https://www.buzzfeed.com/alicelahoda/bathroom-...   \n",
              "8   https://www.buzzfeed.com/adeleb12345/fall-acti...   \n",
              "9   https://www.buzzfeed.com/abbykass/comfortable-...   \n",
              "10  https://www.buzzfeed.com/jasminnina/grown-up-h...   \n",
              "11  https://www.buzzfeed.com/jenae_sitzes/products...   \n",
              "12  https://www.buzzfeed.com/tessafahey/percy-jack...   \n",
              "13  https://www.buzzfeed.com/madisonmcgee/marilyn-...   \n",
              "14  https://www.buzzfeed.com/rachelbrown0213/game-...   \n",
              "15  https://www.buzzfeed.com/watch/video/165020?or...   \n",
              "16  https://www.buzzfeed.com/mychalthompson/nia-lo...   \n",
              "17  https://www.buzzfeed.com/christopherhudspeth/s...   \n",
              "18         /tessafahey/disney-spooky-movie?origin=tuh   \n",
              "19     /tessafahey/celebrity-spelling-test?origin=tuh   \n",
              "20  /claraisabellalaflamme3308/sadie-sink-or-milli...   \n",
              "21  https://www.buzzfeed.com/mjs538/celebs-how-it-...   \n",
              "22  https://www.buzzfeed.com/mayaogolini/horrible-...   \n",
              "23  https://www.buzzfeed.com/swiftliketaylor48/tay...   \n",
              "24  https://www.buzzfeed.com/kaylaharrington/do-re...   \n",
              "25  https://www.buzzfeed.com/chelseastewart/lamar-...   \n",
              "26  https://www.buzzfeed.com/trebkat/spend-a-day-d...   \n",
              "27          https://tasty.co/recipe/simple-kale-chips   \n",
              "28      https://tasty.co/recipe/homemade-dinner-rolls   \n",
              "29  https://tasty.co/recipe/pumpkin-spice-donut-holes   \n",
              "30  https://www.buzzfeed.com/watch/video/160579?or...   \n",
              "31  https://www.buzzfeednews.com/article/elaminabd...   \n",
              "32  https://www.buzzfeed.com/tessafahey/celebrity-...   \n",
              "33  https://www.buzzfeed.com/claraisabellalaflamme...   \n",
              "34  https://www.buzzfeed.com/buzzfeedresearch/civi...   \n",
              "35  https://www.buzzfeed.com/bendzialdowski/dont-w...   \n",
              "\n",
              "                       editor  \n",
              "0                              \n",
              "1                              \n",
              "2                              \n",
              "3              Kelsey Weekman  \n",
              "4               Hannah Marder  \n",
              "5             Victoria Honoré  \n",
              "6            arianatorforever  \n",
              "7                Alice Lahoda  \n",
              "8                 adeleb12345  \n",
              "9                   Abby Kass  \n",
              "10              Jasmin Sandal  \n",
              "11               Jenae Sitzes  \n",
              "12                Tessa Fahey  \n",
              "13              Madison McGee  \n",
              "14            rachelbrown0213  \n",
              "15                             \n",
              "16            Mychal Thompson  \n",
              "17       Christopher Hudspeth  \n",
              "18                Tessa Fahey  \n",
              "19                Tessa Fahey  \n",
              "20  claraisabellalaflamme3308  \n",
              "21               Matt Stopera  \n",
              "22               Maya Ogolini  \n",
              "23          SwiftLikeTaylor90  \n",
              "24           Kayla Harrington  \n",
              "25            Chelsea Stewart  \n",
              "26                    trebkat  \n",
              "27                             \n",
              "28                             \n",
              "29                             \n",
              "30                             \n",
              "31        Elamin Abdelmahmoud  \n",
              "32                Tessa Fahey  \n",
              "33  claraisabellalaflamme3308  \n",
              "34          BuzzFeed Research  \n",
              "35       Benjamin Dzialdowski  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3a87bb4-9c9e-4657-85d8-086f41c83949\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>URL</th>\n",
              "      <th>editor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47 Things You’ll Wish You’d Bought Last Fall B...</td>\n",
              "      <td>https://www.buzzfeed.com/elizabethlilly/things...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28 Actors Who Were Forced To Change Up Their L...</td>\n",
              "      <td>https://www.buzzfeed.com/hannahmarder/makeover...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32 Pictures That Make Me Laugh Until I Pee My ...</td>\n",
              "      <td>https://www.buzzfeed.com/daves4/pictures-that-...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Policy Changes, A Scam, And Child Predator All...</td>\n",
              "      <td>https://www.buzzfeednews.com/article/kelseywee...</td>\n",
              "      <td>Kelsey Weekman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why We All Owe Florence Pugh A Major Apology</td>\n",
              "      <td>https://www.buzzfeed.com/hannahmarder/dont-wor...</td>\n",
              "      <td>Hannah Marder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Here Are 30 Tweets That Made Me Smile This Week</td>\n",
              "      <td>https://www.buzzfeed.com/tori_honore/tweets-st...</td>\n",
              "      <td>Victoria Honoré</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Can You Match The Character To The Nickelodeon...</td>\n",
              "      <td>https://www.buzzfeed.com/arianatorforever/nick...</td>\n",
              "      <td>arianatorforever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>People Are Sharing How They Respond When Someo...</td>\n",
              "      <td>https://www.buzzfeed.com/alicelahoda/bathroom-...</td>\n",
              "      <td>Alice Lahoda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Create An Autumn Bucket List To Find Out Which...</td>\n",
              "      <td>https://www.buzzfeed.com/adeleb12345/fall-acti...</td>\n",
              "      <td>adeleb12345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Reviewers Say They “Live In” These 29 Pieces O...</td>\n",
              "      <td>https://www.buzzfeed.com/abbykass/comfortable-...</td>\n",
              "      <td>Abby Kass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32 Upgrades To Make Your Household Feel More G...</td>\n",
              "      <td>https://www.buzzfeed.com/jasminnina/grown-up-h...</td>\n",
              "      <td>Jasmin Sandal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>40 Products That’ll Totally Change How You Put...</td>\n",
              "      <td>https://www.buzzfeed.com/jenae_sitzes/products...</td>\n",
              "      <td>Jenae Sitzes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>We Used AI To Show What “Percy Jackson” Charac...</td>\n",
              "      <td>https://www.buzzfeed.com/tessafahey/percy-jack...</td>\n",
              "      <td>Tessa Fahey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The Truth About Marilyn Monroe And The Kennedy...</td>\n",
              "      <td>https://www.buzzfeed.com/madisonmcgee/marilyn-...</td>\n",
              "      <td>Madison McGee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Which “GoT” Universe House Do You Belong In?</td>\n",
              "      <td>https://www.buzzfeed.com/rachelbrown0213/game-...</td>\n",
              "      <td>rachelbrown0213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The Cast of Never Have I Ever Take the Co-Star...</td>\n",
              "      <td>https://www.buzzfeed.com/watch/video/165020?or...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Actor Nia Long Responds To Allegations Of Fian...</td>\n",
              "      <td>https://www.buzzfeed.com/mychalthompson/nia-lo...</td>\n",
              "      <td>Mychal Thompson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17 Screenshots Of Bosses That Show How Being I...</td>\n",
              "      <td>https://www.buzzfeed.com/christopherhudspeth/s...</td>\n",
              "      <td>Christopher Hudspeth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>There’s No Way You’ve Seen 25/39 Of These Spoo...</td>\n",
              "      <td>/tessafahey/disney-spooky-movie?origin=tuh</td>\n",
              "      <td>Tessa Fahey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Honestly, It’d Take A Miracle For You To Get 1...</td>\n",
              "      <td>/tessafahey/celebrity-spelling-test?origin=tuh</td>\n",
              "      <td>Tessa Fahey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>You Know If You’re El Or Max, But Are You Sadi...</td>\n",
              "      <td>/claraisabellalaflamme3308/sadie-sink-or-milli...</td>\n",
              "      <td>claraisabellalaflamme3308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>40 Then And Now Pictures Of When Celeb Couples...</td>\n",
              "      <td>https://www.buzzfeed.com/mjs538/celebs-how-it-...</td>\n",
              "      <td>Matt Stopera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>“I Was Never So Happy To Leave A Job In My Lif...</td>\n",
              "      <td>https://www.buzzfeed.com/mayaogolini/horrible-...</td>\n",
              "      <td>Maya Ogolini</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Taylor Swift Has Had Many Iconic Eras, But Onl...</td>\n",
              "      <td>https://www.buzzfeed.com/swiftliketaylor48/tay...</td>\n",
              "      <td>SwiftLikeTaylor90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>I Need Sophie Turner In More Comedies After Se...</td>\n",
              "      <td>https://www.buzzfeed.com/kaylaharrington/do-re...</td>\n",
              "      <td>Kayla Harrington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Lamar Odom Shared His Reaction To Seeing Khloé...</td>\n",
              "      <td>https://www.buzzfeed.com/chelseastewart/lamar-...</td>\n",
              "      <td>Chelsea Stewart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>What Dessert Should You Have Based On How Your...</td>\n",
              "      <td>https://www.buzzfeed.com/trebkat/spend-a-day-d...</td>\n",
              "      <td>trebkat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Simple Kale Chips</td>\n",
              "      <td>https://tasty.co/recipe/simple-kale-chips</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Homemade Dinner Rolls</td>\n",
              "      <td>https://tasty.co/recipe/homemade-dinner-rolls</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Pumpkin Spice Donut Holes</td>\n",
              "      <td>https://tasty.co/recipe/pumpkin-spice-donut-holes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>We Tried The Spiciest Ramen in LA</td>\n",
              "      <td>https://www.buzzfeed.com/watch/video/160579?or...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>“Don’t Worry Darling” Is Actually Just Fine</td>\n",
              "      <td>https://www.buzzfeednews.com/article/elaminabd...</td>\n",
              "      <td>Elamin Abdelmahmoud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Honestly, It’d Take A Miracle For You To Get 1...</td>\n",
              "      <td>https://www.buzzfeed.com/tessafahey/celebrity-...</td>\n",
              "      <td>Tessa Fahey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>You Know If You’re El Or Max, But Are You Sadi...</td>\n",
              "      <td>https://www.buzzfeed.com/claraisabellalaflamme...</td>\n",
              "      <td>claraisabellalaflamme3308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Hey YOU! Got Opinions? Yeah, We Thought So.</td>\n",
              "      <td>https://www.buzzfeed.com/buzzfeedresearch/civi...</td>\n",
              "      <td>BuzzFeed Research</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>“Don’t Worry Darling” Has Caused Quite A Stir ...</td>\n",
              "      <td>https://www.buzzfeed.com/bendzialdowski/dont-w...</td>\n",
              "      <td>Benjamin Dzialdowski</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3a87bb4-9c9e-4657-85d8-086f41c83949')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3a87bb4-9c9e-4657-85d8-086f41c83949 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3a87bb4-9c9e-4657-85d8-086f41c83949');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}