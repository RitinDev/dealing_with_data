{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "B2-IBM_Watson_Natural_Language_API.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/02-WebAPIs/B2-IBM_Watson_Natural_Language_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpOyKprkg78J"
      },
      "source": [
        "# Interacting with the IBM Watson Natural Language Understanding API\n",
        "\n",
        "Another useful API, especially when dealing with text, is the [IBM Watson  Natural Language Understanding API](https://console.bluemix.net/catalog/services/natural-language-understanding), which offers a variety of text analysis functionalities, such as sentiment analysis, entity extraction, keyword extraction, etc.\n",
        "\n",
        "We will give a couple of examples below, to understand how we can take an unstructured piece of text (either the text alone, or a URL with text), and perform some analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Sentiment ana emotion analysis\n",
        "\n",
        "We will first start with the `/analyze` API call ([documentation](https://cloud.ibm.com/apidocs/natural-language-understanding#analyzeget)), which takes as input a piece of text, and returns an analysis across various dimensions. \n",
        "\n",
        "The API supports the following analyses:\n",
        "\n",
        "`categories,classifications,concepts,emotion,entities,keywords,metadata,relations,semantic_roles,sentiment,summarization (experimental),syntax`\n",
        "\n",
        "The API supports not only English, but also a [variety of non-English languages](https://cloud.ibm.com/docs/natural-language-understanding?topic=natural-language-understanding-detectable-languages).\n",
        "\n",
        "In our introductory attempt, we will use the `sentiment` and `emotion` and focus on English texts. \n",
        "\n"
      ],
      "metadata": {
        "id": "w5AdsJAa5iIQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scpvaji2MigH"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o4aBLeBg78M"
      },
      "source": [
        "def analyzeText(text=None, url=None):\n",
        "  '''\n",
        "  This function gets as input either a piece of text or a URL\n",
        "  and then contacts the IBM Watson NLU API to perform sentiment\n",
        "  and emotion analysis on the text/url.\n",
        "  '''\n",
        "\n",
        "  # Pointers to the IBM Server that we will use for the\n",
        "  # endpointn and the API key provided by IBM. \n",
        "  # This is my own, personal IBM_SERVER_URL and IBM_API_KEY\n",
        "  # \n",
        "  # You probably should can register and get your own credentials\n",
        "  # The ones below have a quota of 1000 calls per day \n",
        "  # and can run out quickly if multiple people use these\n",
        "  IBM_SERVER_URL = 'https://api.us-east.natural-language-understanding.watson.cloud.ibm.com/instances/7955e520-ee3e-4435-917f-f2e626d7b297'\n",
        "  IBM_API_KEY = 'ipYHoxag1pB7bkURAGAnZUaOstg05h8_jsgL4io_Co1l'\n",
        "\n",
        "  endpoint = f\"{IBM_SERVER_URL}/v1/analyze\"\n",
        "  username = \"apikey\"\n",
        "  password = IBM_API_KEY\n",
        "  \n",
        "  parameters = {\n",
        "      'features': 'emotion,sentiment',\n",
        "      'version' : '2022-04-07',\n",
        "      'text': text,\n",
        "      'language' : 'en',\n",
        "      'url' : url # this is an alternative to sending the text\n",
        "  }\n",
        "\n",
        "  resp = requests.get(endpoint, params=parameters, auth=(username, password))\n",
        "  \n",
        "  return resp.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCGGi4Sug78U"
      },
      "source": [
        "# We will analyze the text below using the IBM Watson API\n",
        "\n",
        "review = '''\n",
        "I got their Egg & Cheese sandwich on a Whole Wheat Everything Bagel. \n",
        "First off, I loved loved loved the texture of the bagel itself. \n",
        "It was very chewy yet soft, which is a top feature for a NY style bagel. \n",
        "However, I thought there could've been more seasoning on top of \n",
        "the bagel as I found the bagel itself to be a bit bland. \n",
        "\n",
        "Speaking of bland, I thought the egg and cheese filling were also quite bland. \n",
        "This was definitely lacking salt and pepper in the eggs and the cheese didn't\n",
        "really add too much flavor either, which was really disappointing! \n",
        "My mom also had the same complaint with her bagel sandwich \n",
        "(she had the egg sandwich on a blueberry bagel) so I definitely wasn't \n",
        "the only one.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRYAW9WHg78Y"
      },
      "source": [
        "data = analyzeText(text=review)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HofZ85yFg78c"
      },
      "source": [
        "Now, let's try to understand the structure of the answer. First, we check the high-level keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD3qoCmMg78d"
      },
      "source": [
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itoPJcaNg78h"
      },
      "source": [
        "Now, let's check the content of these keys:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewzjsNHg78i"
      },
      "source": [
        "data['language']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEMFHA_mg78n"
      },
      "source": [
        "data['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UO7k-scg78r"
      },
      "source": [
        "data['emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdBxICpHg78w"
      },
      "source": [
        "# Let's go deeper into the 'emotion' dictionary\n",
        "data['emotion']['document']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-A1ZJFyg780"
      },
      "source": [
        "# And a bit more\n",
        "data['emotion']['document']['emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19-vgKPag785"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Type your own piece of text, and analyze it to extract sentiment and emotions. Discuss your findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO2SHOE7g786"
      },
      "source": [
        "my_own_text = '''\n",
        "'''\n",
        "\n",
        "analyzeText( text = my_own_text )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxKeCado_5EU"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Below is slightly different call, which takes as input a URL to analyze, instead of a piece of text. Use it to analyze a URL of your choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSiHLtpWADP-"
      },
      "source": [
        "news_url = 'https://www.wsj.com/articles/june-jobs-report-unemployment-rate-economy-growth-2022-11657237512'\n",
        "analyzeText(url = news_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFIwBAw9g789"
      },
      "source": [
        "## Entities call\n",
        "\n",
        "The code below changes slightly the way that we way that we call the API. Instead of asking for sentiment and emotion, we instead ask to extract entities from the text, and also the sentiment and emotion for each of these entities.\n",
        "\n",
        "In terms of natural language processing, we will examine a couple of capabilities of the API. First, you will see that there is the capability of \"normalizing\" each entity, so that two different ways of saying the same thing get mapped to the same entity. So for example, \"President Trump\" and \"Donald Trump\" get mapped to the same Knowledge Graph entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractEntities(text=None, url=None):\n",
        "\n",
        "    IBM_SERVER_URL = 'https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/9e683088-0d12-4399-8118-518f3e60e8c4'\n",
        "    IBM_API_KEY = 'yx39wyiwPNGm7DoDUPCSJB4SzFkr0qurARfbGYyEdaoC'\n",
        "\n",
        "    endpoint = f\"{IBM_SERVER_URL}/v1/analyze\"\n",
        "    username = \"apikey\"\n",
        "    password = IBM_API_KEY\n",
        "    \n",
        "    parameters = {\n",
        "        'features': 'entities',\n",
        "        'version' : '2022-04-07',\n",
        "        'entities.limit' : 10,\n",
        "        'entities.sentiment' : True,\n",
        "        'entities.emotion' : True,\n",
        "        'text': text,\n",
        "        'language' : 'en',\n",
        "        'url' : url # this is an alternative to sending the text\n",
        "    }\n",
        "\n",
        "    resp = requests.get(endpoint, params=parameters, auth=(username, password))\n",
        "    \n",
        "    return resp.json()"
      ],
      "metadata": {
        "id": "OHqTURmF8pFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCfu7Mp8g79F"
      },
      "source": [
        "news_url = 'https://www.nytimes.com/2022/05/21/world/europe/kirill-putin-russian-orthodox-church.html'\n",
        "\n",
        "data = extractEntities(url=news_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m1uBsl4M6WL"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXcFMzpSg79L"
      },
      "source": [
        "# Let's see what we get back as top-level attributes\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V0VyAAtSg79T"
      },
      "source": [
        "# Let' see the entities list\n",
        "data[\"entities\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X5VaaqCg79X"
      },
      "source": [
        "# Let' see the 7th entity. Notice the \"disambiguated\" attribute that\n",
        "# points to \"canonical\" versions of the entity, in DBPedia\n",
        "# Notice that \"Patriarch Kirill\"\" is the actual term used in the text\n",
        "data[\"entities\"][6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's put the results in a dataframe, so that we can browse easier\n",
        "import pandas as pd\n",
        "\n",
        "pd.json_normalize(data['entities'])"
      ],
      "metadata": {
        "id": "R3wxxcJe_j_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "PVct5ns8g79m"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "* First of all, **get your own credentials for the IBM Watson API**. The demo key that we use above has a limited quota.\n",
        "* Use an API to get news articles. \n",
        "    * Option 1: Use the API at https://newsapi.org to fetch the news from various sources. Print the entities that are currently being discussed in the news, together with their relevance value and the associated sentiment.\n",
        "    * Option 2: Use the NY Times API to fetch the Top Stories News. You can register and get an API key at https://developer.nytimes.com/. The `Top Stories V2 API` provides the details of the news of the day: (The API call documentation is at https://developer.nytimes.com/docs/top-stories-product/1/overview and the API Call is  https://api.nytimes.com/svc/topstories/v2/home.json?api-key=PUTYOURKEYHERE). Repeat the entity extraction process from above.\n",
        "    * Option 3: Use the Guardian API at https://open-platform.theguardian.com/documentation/ to fetch news from The Guardian.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vylSuCdkg79n"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}