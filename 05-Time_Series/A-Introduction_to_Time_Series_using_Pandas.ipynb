{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/05-Time_Series/A-Introduction_to_Time_Series_using_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJXQvkmz9pmk"
      },
      "source": [
        "# Introduction to Time Series and Forecasting\n",
        "\n",
        "*Based on the book [Introduction to Time Series and Forecasting](https://link.springer.com/book/10.1007/978-3-319-29854-2) by Brockwell and Davis.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCYUqz2k9pmo"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "!sudo pip install -U -q PyMySQL sqlalchemy sql_magic \n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Time Series?\n",
        "\n",
        "A time series is a set of observations $x_t$, each one being recorded at a specific time $t$. In our sessions, we focus on **_discrete_** time series, where observations are recorded at fixed time intervals (e.g., once an hour, or every 30 seconds, or every 7 days). \n",
        "\n",
        "We only consider **_regular_** time series, where we the time between observations is constant (i.e., we do not consider account deposits or withdrawals from an ATM that happen at various times; these are examples of an irregular time series)."
      ],
      "metadata": {
        "id": "D79hThX7_yOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrymQoHL9pmn"
      },
      "source": [
        "## Examples of Time Series\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB301JbH9pmq"
      },
      "source": [
        "### Australian red wine \"sales\", (thousands of litres) monthly, Jan 80 - Oct 91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzhItBE9pmq"
      },
      "source": [
        "The file [`australian-wine-sales.txt`](https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt) contains the monthly sales of Australian red wines in for the period Jan-1980 to Oct-1991. Let's take a peak at the data file: We will use Pandas and the `pd.read_csv` function to read the text file into a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ybvQax9pmt"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "##### What is this code?\n",
        "# The `read_csv` command can read directly from a URL, so we pass directly \n",
        "# the URL of the dataset as a parameter. Also, since the file uses the \n",
        "# tab character to separate the columns, we pass the `sep='\\t'` option to \n",
        "# the `read_csv` command, indicating that the separator is the \"tab\"\n",
        "# (i.e. `\\t` ) character.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDLr3O019pmt"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try to plot the time series:"
      ],
      "metadata": {
        "id": "gxVWM-gADlLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmOwjJ_b9pmu"
      },
      "outputs": [],
      "source": [
        "df.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLiDOYdE9pmv"
      },
      "source": [
        "The plot would look better if we had the x-axis to be a date, instead of a number. By default, Pandas uses the \"index\" of the dataframe as the x-axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5fv2f79pmt"
      },
      "source": [
        "Let's check the data types that Pandas inferred:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "MM9oQ1h3Ct0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLNvQRP9pmu"
      },
      "source": [
        "In this case pandas figured out that `Sales` is a number, but not that `Date` is a date. The following two commands convert the two columns into a date and a numeric data type, respectively. (Technically, we could skip the conversion for `Sales` but I want to show how we convert data types.)\n",
        "\n",
        "We will set  the `Date` column to be the index of the dataframe, so that we can plot the sales with dates as the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df[\"Sales\"] = pd.to_numeric(df[\"Sales\"])"
      ],
      "metadata": {
        "id": "wrb5THMgDPIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkFv-E9-9pmv"
      },
      "outputs": [],
      "source": [
        "df = df.set_index(\"Date\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot()"
      ],
      "metadata": {
        "id": "5aFxxILeE_Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VphKhVNF9pmw"
      },
      "source": [
        "It appears from the graph that the sales have an upward trend and a seasonal pattern with a peak in July and a trough in January."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical questions in Time Series Analysis\n",
        "\n",
        "* What is the overall trend?\n",
        "* Is the current month above or below expectations?\n",
        "* Do we observe any anomalies in our data?\n",
        "* What should we expect for next month? Next year?\n",
        "\n",
        "To answer such questions, we typically start by considering a few **models** of time series, and see how well such models capture the behavior of the data that we have. \n",
        "\n",
        "For example, it is important to recognize the presence of **seasonal components** and to remove them so as not to confuse them with long-term trends. This process is known as **seasonal adjustment**.\n",
        "\n"
      ],
      "metadata": {
        "id": "KI7CrL9Ub2ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Time Series"
      ],
      "metadata": {
        "id": "Azx22NzVZIJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NYC Accidents\n",
        "\n",
        "Taking a look at the aggregate data for NYC accidents, we can start asking questions like:\n",
        "\n",
        "* Does the trend look good?\n",
        "* When do we observe spikes in accidents on a daily and weekly basis, so that we can deploy resources accordingly? (We can also do that on a geographical basis, but we will examine that separately)\n",
        "* Which dates or times are unusually high or low?"
      ],
      "metadata": {
        "id": "nj953FOng8uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/?charset=utf8mb4'.format(\n",
        "    host = 'db.ipeirotis.org', \n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2015', \n",
        "    encoding = 'utf8mb4')\n",
        "mysql_conn = create_engine(conn_string).connect()\n",
        "\n",
        "# Get the number of accidents per hour\n",
        "sql = '''\n",
        "  SELECT date_format(DATE_TIME,'%%Y-%%m-%%d %%H:00') AS acc_date, COUNT(*) AS accidents \n",
        "  FROM collisions.collisions \n",
        "  GROUP BY date_format(DATE_TIME,'%%Y-%%m-%%d %%H:00')\n",
        "  ORDER BY date_format(DATE_TIME,'%%Y-%%m-%%d %%H:00')\n",
        "'''\n",
        "\n",
        "# Read the results in Pandas\n",
        "acc_hourly = pd.read_sql(sql, con=mysql_conn)\n",
        "\n",
        "# Convert the acc_date column to datetime\n",
        "acc_hourly['acc_date'] = pd.to_datetime(acc_hourly['acc_date'])\n",
        "\n",
        "acc_hourly.plot(\n",
        "    kind='line',\n",
        "    x='acc_date',\n",
        "    y='accidents'\n",
        ")"
      ],
      "metadata": {
        "id": "o8J6P2kBZNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can change the granularity of the time series by calculating aggregate statistics:"
      ],
      "metadata": {
        "id": "SsqyzJbjifGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### NYC Accidents Daily/Weekly/Monthly\n",
        "\n",
        "acc_aggregated = acc_hourly.copy()\n",
        "acc_aggregated = acc_aggregated.set_index('acc_date')\n",
        "\n",
        "# Modify the aggregation here to get values for daily, weekly, monthly, etc\n",
        "acc_aggregated = acc_aggregated.resample('1W').sum()\n",
        "\n",
        "# Plot the aggregated data; by default, x-axis is the index\n",
        "acc_aggregated.plot(\n",
        "    kind='line',\n",
        "    y='accidents'\n",
        ")"
      ],
      "metadata": {
        "id": "Yx4DqcGMiSrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The monthly accidental deaths data, 1973–1978\n",
        "\n",
        "For more details on the data manipulation aspect of the commands below, see the [Reading Data in Pandas](https://github.com/ipeirotis/dealing_with_data/blob/master/03-Pandas/B1-Pandas_Reading_Data.ipynb) notebook, in the section \"_Loading a 'Fixed Width' file_\"."
      ],
      "metadata": {
        "id": "dBCHsy0EjZuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deaths = pd.read_fwf(\"https://storage.googleapis.com/datasets_nyu/acc-deaths.txt\")\n",
        "deaths = pd.melt(deaths, id_vars=['Year'], \n",
        "        value_vars=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
        "        var_name='Month', value_name='Deaths')\n",
        "deaths[\"Date\"] = deaths[\"Month\"] + \"-\" + deaths[\"Year\"].astype(str)\n",
        "deaths[\"Date\"] = pd.to_datetime(deaths[\"Date\"], format='%b-%Y')\n",
        "deaths = deaths.drop([\"Month\",\"Year\"], axis='columns')\n",
        "deaths = deaths.set_index(keys=\"Date\")\n",
        "deaths = deaths.sort_index()\n",
        "deaths.plot()"
      ],
      "metadata": {
        "id": "VBh9fGmIgtYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### US Population"
      ],
      "metadata": {
        "id": "tSxxbhu1pb_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the thousands=',' option to properly convert the population numbers to integers\n",
        "population = pd.read_csv(\"https://storage.googleapis.com/datasets_nyu/us-population.txt\", sep=' ', thousands=',')\n",
        "population[\"Year\"] = pd.to_numeric(population[\"Year\"])\n",
        "population[\"US_Population\"] = pd.to_numeric(population[\"US_Population\"])\n",
        "\n",
        "population.plot(\n",
        "    kind = 'line',\n",
        "    x = 'Year',\n",
        "    y = 'US_Population',\n",
        "    marker = 'o'\n",
        ")"
      ],
      "metadata": {
        "id": "5tyB5wfzkU_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l3fA0m8vpZVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stock prices\n",
        "\n",
        "If you are interested in downloading stock information, [this article contains a good discussion](https://towardsdatascience.com/a-comprehensive-guide-to-downloading-stock-prices-in-python-2cd93ff821d4)."
      ],
      "metadata": {
        "id": "HM6vYcs_pZuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "3XLXdIT3o9xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "stock_df = yf.download(tickers = ['GOOG','AAPL','MSFT'],\n",
        "                       interval = '1d', # download daily prices\n",
        "                       start='2005-01-01', # fetch prices after 2004\n",
        "                       auto_adjust = True, # adjust for splits etc\n",
        "                       progress = False # do not show a progress bar\n",
        "                       )\n",
        "stock_df"
      ],
      "metadata": {
        "id": "Y17fnaQJo6Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.plot(\n",
        "    y = 'Close',\n",
        "    title=\"Daily Closing price\",\n",
        "    logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "lfALYjjkpO3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variation of the plot, where we normalize all stocks\n",
        "# by dividing the stock with the price on the first day\n",
        "# of 2005.\n",
        "normalized_stock_df = stock_df['Close'].copy()\n",
        "first_date = normalized_stock_df.index[0]\n",
        "first_entry = normalized_stock_df.loc[first_date]\n",
        "normalized_stock_df = normalized_stock_df / first_entry\n",
        "\n",
        "normalized_stock_df.plot(\n",
        "    title=\"Normalized Price (y=1 is at Jan 1 2005)\",\n",
        "    logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "YQptrr7YstkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gFoX-VUHs2u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_stock_df.index[0]"
      ],
      "metadata": {
        "id": "_9zhuozrs8nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Time Series: Autocorrelation"
      ],
      "metadata": {
        "id": "5nysr3TywnJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A commonly analyzed property of a time series is the **autocorrelation** of the sequential observations. Simply stated, a high degree of autocorrelation means that if we know the value at time $t$, we can predict well the value at $t+1$.\n",
        "\n",
        "Using the `autocorr` function, we estimate the autocorrelation of our `Sales` time series:"
      ],
      "metadata": {
        "id": "PVV7Wv-gwwkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr()"
      ],
      "metadata": {
        "id": "TpIx6XNQxBbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of autocorrelation can extend to longer time periods, and not just to $t$ and $t+1$. We can extract autocorrelation for various **lag** values."
      ],
      "metadata": {
        "id": "o1S-QwxLxgUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr(lag=1) # same as simply df[\"Sales\"].autocorr()"
      ],
      "metadata": {
        "id": "RG_8UkVpx7bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between t and t+2\n",
        "# ie sales now and 2 months later\n",
        "df[\"Sales\"].autocorr(lag=2)"
      ],
      "metadata": {
        "id": "F0eYRnNExBer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between t and t+3\n",
        "# ie sales now and 3 months later\n",
        "df[\"Sales\"].autocorr(lag=3)"
      ],
      "metadata": {
        "id": "voqmEUsbIXwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNpq-WQk9pmw"
      },
      "source": [
        "### Lag plots and  autocorrelation plots\n",
        "\n",
        "Pandas provides two types of plots that can be used for the analysis of time series: the `lag_plot` and the `autocorrelation_plot`. We can also use the seasonal decomposition functionality of `statsmodels` to separate the time series into a trend, seasonal component, and residual noise. We will go quickly over these for now, mainly for demo purposes. Proper treatment of these topics require deeper analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8gIodj9pmw"
      },
      "source": [
        "#### Lag plot\n",
        "\n",
        "By default, the lag plot shows the value of the series at time $t$ vs. its value at time $t+1$. If there is no dependency (i.e., the time series is noise) then the lag plot is a scatterplot without any sign of correlation. If we can see a pattern and a correlation, then the series exhibits autocorrelation. For example, below we can see that there is a rather strong correlation of the two variables, indicating that the sales in time $t+1$ is similar to the sales at time $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U1MAtJ9pmw"
      },
      "outputs": [],
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 1, c='b')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The plot above shows the autocorrelation of the t and t+1\n",
        "# is around 0.73\n",
        "df[\"Sales\"].autocorr(lag=1)"
      ],
      "metadata": {
        "id": "yo3Mbmp2zCFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the lag plot, where we plot $t$ and $t+12$. Notice that we have a higher correlation (less spread out points)"
      ],
      "metadata": {
        "id": "v8hotHdoyq5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 12, c='r')"
      ],
      "metadata": {
        "id": "cSpqApSYyofW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr(lag=12)"
      ],
      "metadata": {
        "id": "5DABBWdSy5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the two of them together."
      ],
      "metadata": {
        "id": "Ygo-Me32zV1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 1, c='b')\n",
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 12, c='r')"
      ],
      "metadata": {
        "id": "Sw6dQj20zRXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axNcEjbR9pmx"
      },
      "source": [
        "#### Autocorrelation Plot\n",
        "\n",
        "In a more general setting, we want to also see if the value of the series at time $t$ is predictive of the value at time $t+n$. Such dependency would indicate that there is *autocorrelation* in the series. The autocorrelation plot shows the correlation value for various values of $n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmk93mL9pmy"
      },
      "outputs": [],
      "source": [
        "pd.plotting.autocorrelation_plot(df[\"Sales\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above, with the oscillating autocorrelation values indicate that there is a **seasonality** component in the time series. (As we see that the correlation in 12-month increments to go up and then down.) \n",
        "\n",
        "Let's see next how we can extract the seasonal component."
      ],
      "metadata": {
        "id": "5GIfjYKWoPqs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmir3VtH9pmy"
      },
      "source": [
        "## Trend and Seasonal Decomposition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGClDRKA9pmz"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# We decompose assumming a 12-month periodicity. \n",
        "# We can also specify a multiplicative instead of an additive model\n",
        "# The additive model is Y[t] = T[t] + S[t] + e[t]\n",
        "# The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
        "decomposition = seasonal_decompose(df['Sales'], model='multiplicative', freq=12, extrapolate_trend='freq')  \n",
        "fig = plt.figure()\n",
        "fig = decomposition.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxIe9Gjc9pmz"
      },
      "source": [
        "#### Accessing indinvidual components of the decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jA9GC049pm0"
      },
      "source": [
        "Once we have the decomposed time series model, we can also access the different components.\n",
        "\n",
        "For example, we can get the trend of the time series, after removing the seasonality component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHhcWhrM9pm0"
      },
      "outputs": [],
      "source": [
        "# The outcome is a pandas Series, which is effectively the same as a single column of dataframe\n",
        "decomposition.trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr8pLOvd9pm0"
      },
      "outputs": [],
      "source": [
        "decomposition.trend.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8bJ5RN9pm5"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "* Below we fetch the daily number of vehicular accidents in NYC.\n",
        "* Examine the autocorrelation structure of the accidents.\n",
        "* Perform a decomposition of the time series into a trend, seasonal, and residual component.\n",
        "* Try out both the additive and the multiplicative approach for the decomposition. Try to interpret and understand the difference in the reported seasonal component.\n",
        "* Instead of counting accidents, extract the number of injuries and perform the same analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip install -U -q PyMySQL sqlalchemy sql_magic "
      ],
      "metadata": {
        "id": "M4sofXooGHDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/{db}?charset=utf8mb4'.format(\n",
        "    host = 'db.ipeirotis.org', \n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2015', \n",
        "    db = 'collisions',\n",
        "    encoding = 'utf8mb4')\n",
        "\n",
        "mysql_conn = create_engine(conn_string).connect()\n",
        "\n",
        "sql = '''\n",
        "  SELECT date_format(DATE_TIME,'%%Y-%%m-%%d') AS acc_date, COUNT(*) AS accidents \n",
        "  FROM collisions.collisions \n",
        "  GROUP BY date_format(DATE_TIME,'%%Y-%%m-%%d')\n",
        "  ORDER BY date_format(DATE_TIME,'%%Y-%%m-%%d')\n",
        "'''\n",
        "\n",
        "acc = pd.read_sql(sql, con=mysql_conn)\n",
        "acc['acc_date'] = pd.to_datetime(acc['acc_date'])\n",
        "acc = acc.set_index('acc_date').resample('1D').sum()"
      ],
      "metadata": {
        "id": "rpVw1x6dEv0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc.plot()"
      ],
      "metadata": {
        "id": "sVEIpRawHxCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "52PxGdwTKoYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# We decompose assumming a 12-month periodicity. \n",
        "# We can also specify a multiplicative instead of an additive model\n",
        "# The additive model is Y[t] = T[t] + S[t] + e[t]\n",
        "# The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
        "\n",
        "period = 365 # We have daily observations, and we consider one year (365 days) \n",
        "             # as the maximum seasonality period\n",
        "decomposition = seasonal_decompose(acc['accidents'], \n",
        "                                   model='mutiplicative', \n",
        "                                   freq=period, \n",
        "                                   extrapolate_trend='freq'\n",
        "                )  \n",
        "fig = decomposition.plot()"
      ],
      "metadata": {
        "id": "FWuEiHotJhuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trend\n",
        "decomposition.trend.plot()"
      ],
      "metadata": {
        "id": "gyQqunWlm6NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the seasonal component for the first year of data\n",
        "decomposition.seasonal[0:period].plot()"
      ],
      "metadata": {
        "id": "YbaLAhL8J52l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly8PGn_k9pm6"
      },
      "source": [
        "## Advanced: Time Series Window operations: Rolling / Expanding / EW\n",
        "\n",
        "One question that comes up when we have a periodic time series is: \"How can I figure out the overall trend?\". In the examples above, we relied on a \"black box\" where we simply asked for the time series to be decomposed into a trend, seasonal, and residual component. Now, let's dig a bit deeper on how we can extract trend components that are unaffected by seasonality.\n",
        "\n",
        "For that, we often rely on \"window\" functions, that operate over a set of continuous time series points. For example, if we have a time series that has a 12-month seasonality, we can take the 12-month average, which will not exhibit seasonality, but will capture the trend. \n",
        "\n",
        "These windows functions are common time series operations. Pandas provides support for various types of windows. Here are a few that are commonly used: \n",
        "* [Rolling window](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html): We compute the function over a time period equal to a window\n",
        "* [Expanding](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.expanding.html): We compute the function over a period of 1, 2, 3,... instances.\n",
        "* [Exponential weighting](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html): We keep a window of a fixed size but we weight less and less (exponentially) the old data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTHQl5QI9pm6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqelKNEv9pm7"
      },
      "outputs": [],
      "source": [
        "# Use different linestyle, and use high alpha to make the series less visually prominent\n",
        "df['Sales'].plot(label='Raw', linestyle =\"--\", alpha=0.25)\n",
        "\n",
        "# Plot the 12-month moving average\n",
        "df['Sales'].rolling(12).mean().plot(label='12M MA', alpha=0.75)\n",
        "\n",
        "# Plot the expanding mean. This is the mean of the series from the beginning till that point in time\n",
        "df['Sales'].expanding().mean().plot(label='Expanding', alpha=0.75)\n",
        "\n",
        "# Plot the exponentially weighted moving average. This moving average weighs more heavily the newer\n",
        "# data points and weighs less the old ones. \n",
        "df['Sales'].ewm(halflife=12).mean().plot(label='EWMA (halflife 12M)', alpha=0.75)\n",
        "\n",
        "# places the legend to the right side (1) and middle of the y-axis (0.5)\n",
        "plt.legend(bbox_to_anchor=(1, .5)) \n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WDKeL7M9pm-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2OUqF2F9pm_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "D-Pandas_and_Time_Series.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}